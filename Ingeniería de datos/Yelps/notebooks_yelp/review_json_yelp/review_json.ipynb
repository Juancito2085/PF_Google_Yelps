{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias a usar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from multiprocessing import Pool, cpu_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leer el archivo json\n",
    "\n",
    "chunks = pd.read_json('/Users/luisalbertocerelli/Desktop/00-Todo/Data_Science/01-Full_Time/TPFINAL/Yelp/notebooks_yelp/review_json_yelp/review.json', lines=True, chunksize=1000)\n",
    "\n",
    "df_review_crudo = pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capacidad del archivo \"review.json\" es de 5341.868833 MB\n"
     ]
    }
   ],
   "source": [
    "#Analizar el 'peso' del archivo\n",
    "\n",
    "import os\n",
    "\n",
    "size_in_bytes = os.path.getsize('/Users/luisalbertocerelli/Desktop/00-Todo/Data_Science/01-Full_Time/TPFINAL/Yelp/notebooks_yelp/review_json_yelp/review.json')\n",
    "size_in_megabytes = size_in_bytes / 1e6\n",
    "\n",
    "print(f'La capacidad del archivo \"review.json\" es de {size_in_megabytes} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6990280, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_crudo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6990280 entries, 0 to 6990279\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Dtype         \n",
      "---  ------       -----         \n",
      " 0   review_id    object        \n",
      " 1   user_id      object        \n",
      " 2   business_id  object        \n",
      " 3   stars        int64         \n",
      " 4   useful       int64         \n",
      " 5   funny        int64         \n",
      " 6   cool         int64         \n",
      " 7   text         object        \n",
      " 8   date         datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(4), object(4)\n",
      "memory usage: 480.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_review_crudo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_id      0\n",
       "user_id        0\n",
       "business_id    0\n",
       "stars          0\n",
       "useful         0\n",
       "funny          0\n",
       "cool           0\n",
       "text           0\n",
       "date           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verificar si hay valores nulos\n",
    "df_review_crudo.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar si hay valores duplicados\n",
    "df_review_crudo.duplicated().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de las columnas: \n",
    "\n",
    "Segun el estudio realizado, nos encontramos con tres columnas en particular que se escapan a nuestro objetivo, las mismas son: \n",
    "\n",
    "<u>useful (int64):</u> Es el número de veces que otros usuarios han marcado la reseña como \"útil\". Este valor es un número entero.<span style='color:lightpink'>consideramos que no es necesaria a nuestros intereses</span><span style='color:red'> SE PROCEDE A BORRAR LA COLUMNA</span>\n",
    "\n",
    "<u>funny (int64):</u>  Es el número de veces que otros usuarios han marcado la reseña como \"divertida\". Este valor es un número entero.<span style='color:lightpink'>consideramos que no es necesaria a nuestros intereses</span><span style='color:red'> SE PROCEDE A BORRAR LA COLUMNA</span>\n",
    "\n",
    "<u> cool (int64): </u> Es el número de veces que otros usuarios han marcado la reseña como \"genial\" o \"interesante\". Este valor es un número entero.<span style='color:lightpink'>consideramos que no es necesaria a nuestros intereses</span><span style='color:red'> SE PROCEDE A BORRAR LA COLUMNA</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacemos una copia del dataframe original\n",
    "df_review = df_review_crudo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6990280, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verificamos la copia\n",
    "df_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y dropeamos las columnas que de 'useful', 'funny' y 'cool' que no vamos a usar\n",
    "df_review.drop(columns=['useful', 'funny', 'cool'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6990280, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verificamos que se hayan dropeado\n",
    "df_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/luisalbertocerelli/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#Funcion para analizar sentimientos utilizando la libreria 'multiprocessing' la cual, en las lineas 29 y 32 prepara la maquina para utilizar todos los nucleos de procesamiento disponibles, menos uno, para realizar la tarea de forma paralela y lo mas rapido posible.\n",
    "\n",
    "import multiprocess as mp\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def analisis_sentimientos(texto):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(texto)\n",
    "\n",
    "# Function to parallelize the sentiment analysis\n",
    "def parallelize_dataframe(df, func, num_cores):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = mp.Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def apply_sentiment_analysis(df):\n",
    "    df['sentimientos'] = df['text'].apply(analisis_sentimientos)\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load your DataFrame\n",
    "    # df_review = pd.read_csv('path_to_your_file.csv') # Uncomment this if loading from a file\n",
    "    \n",
    "    # Obtener el número total de núcleos de procesamiento lógicos en la máquina\n",
    "    total_cores = mp.cpu_count()\n",
    "\n",
    "    # Usar todos menos uno para dejar recursos libres para otras tareas\n",
    "    num_cores_to_use = total_cores - 1\n",
    "\n",
    "    # Paralelizar el DataFrame\n",
    "    df_review = parallelize_dataframe(df_review, apply_sentiment_analysis, num_cores=num_cores_to_use)\n",
    "\n",
    "    # Guardar el resultado si es necesario\n",
    "    # df_review.to_csv('path_to_save_result.csv', index=False) # Uncomment to save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacemos un csv de las primeras 1000 filas para ver como quedo\n",
    "df_review.head(1000).to_csv('/Users/luisalbertocerelli/Desktop/00-Todo/Data_Science/01-Full_Time/TPFINAL/Yelp/notebooks_yelp/review_json_yelp.df_review_final_1000_filas.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos el df en un nuevo archivo parquet\n",
    "df_review.to_parquet('/Users/luisalbertocerelli/Desktop/00-Todo/Data_Science/01-Full_Time/TPFINAL/Yelp/notebooks_yelp/review_json_yelp/review_listo_para_usar.parquet', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capacidad del archivo parquet \"review_listo_para_usar\" es de 2999.105605 MB\n"
     ]
    }
   ],
   "source": [
    "#Analizamos el 'peso' del nuevo archivo parquet\n",
    "import os\n",
    "\n",
    "size_in_bytes = os.path.getsize('/Users/luisalbertocerelli/Desktop/00-Todo/Data_Science/01-Full_Time/TPFINAL/Yelp/notebooks_yelp/review_json_yelp/review_listo_para_usar.parquet')\n",
    "size_in_megabytes = size_in_bytes / 1e6\n",
    "\n",
    "print(f'La capacidad del archivo parquet \"review_listo_para_usar\" es de {size_in_megabytes} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La diferencia de peso entre el archivo json y el parquet es de 2342.7632280000003 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Obtén el tamaño del archivo JSON\n",
    "size_in_bytes_json = os.path.getsize('/Users/luisalbertocerelli/Desktop/00-Todo/Data_Science/01-Full_Time/TPFINAL/Yelp/notebooks_yelp/review_json_yelp/review.json')\n",
    "size_in_megabytes_json = size_in_bytes_json / 1e6\n",
    "\n",
    "# Obtén el tamaño del archivo Parquet\n",
    "size_in_bytes_parquet = os.path.getsize('/Users/luisalbertocerelli/Desktop/00-Todo/Data_Science/01-Full_Time/TPFINAL/Yelp/notebooks_yelp/review_json_yelp/review_listo_para_usar.parquet')\n",
    "size_in_megabytes_parquet = size_in_bytes_parquet / 1e6\n",
    "\n",
    "# Calcula la diferencia\n",
    "diferencia = size_in_megabytes_json - size_in_megabytes_parquet \n",
    "\n",
    "print(f'La diferencia de peso entre el archivo json y el parquet es de {diferencia} MB')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
